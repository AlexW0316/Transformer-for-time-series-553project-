{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNYPa9zE4BaPvyV6XxhiCp9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mziycfh/Transformers4TimeSeries/blob/main/Copy_of_Untitled0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8fsbqwCMXtI",
        "outputId": "fbd4f098-9dea-442b-e81e-02935a77fa86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Autoformer'...\n",
            "remote: Enumerating objects: 371, done.\u001b[K\n",
            "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 371 (delta 155), reused 143 (delta 143), pack-reused 162 (from 1)\u001b[K\n",
            "Receiving objects: 100% (371/371), 2.20 MiB | 18.33 MiB/s, done.\n",
            "Resolving deltas: 100% (219/219), done.\n",
            "/content/Autoformer\n"
          ]
        }
      ],
      "source": [
        "# Clone the Autoformer repository\n",
        "!git clone https://github.com/thuml/Autoformer.git\n",
        "%cd Autoformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t8U6g_W9Mmez",
        "outputId": "2a0e5fcd-4a19-4afd-886c-5f50019d6158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (1.5.2)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (1.26.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.8.0)\n",
            "Collecting reformer_pytorch (from -r requirements.txt (line 6))\n",
            "  Downloading reformer_pytorch-1.4.4-py3-none-any.whl.metadata (764 bytes)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 1)) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->-r requirements.txt (line 2)) (3.5.0)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 3)) (2.5.1+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->-r requirements.txt (line 3)) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision->-r requirements.txt (line 3)) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision->-r requirements.txt (line 3)) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision->-r requirements.txt (line 3)) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision->-r requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision->-r requirements.txt (line 3)) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision->-r requirements.txt (line 3)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 5)) (3.2.0)\n",
            "Collecting axial-positional-embedding>=0.1.0 (from reformer_pytorch->-r requirements.txt (line 6))\n",
            "  Downloading axial_positional_embedding-0.2.1.tar.gz (2.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from reformer_pytorch->-r requirements.txt (line 6)) (0.8.0)\n",
            "Collecting local-attention (from reformer_pytorch->-r requirements.txt (line 6))\n",
            "  Downloading local_attention-1.9.15-py3-none-any.whl.metadata (683 bytes)\n",
            "Collecting product-key-memory (from reformer_pytorch->-r requirements.txt (line 6))\n",
            "  Downloading product_key_memory-0.2.11-py3-none-any.whl.metadata (717 bytes)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 1)) (1.16.0)\n",
            "Collecting colt5-attention>=0.10.14 (from product-key-memory->reformer_pytorch->-r requirements.txt (line 6))\n",
            "  Downloading CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision->-r requirements.txt (line 3)) (3.0.2)\n",
            "Downloading reformer_pytorch-1.4.4-py3-none-any.whl (16 kB)\n",
            "Downloading local_attention-1.9.15-py3-none-any.whl (9.0 kB)\n",
            "Downloading product_key_memory-0.2.11-py3-none-any.whl (6.5 kB)\n",
            "Downloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
            "Building wheels for collected packages: axial-positional-embedding\n",
            "  Building wheel for axial-positional-embedding (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for axial-positional-embedding: filename=axial_positional_embedding-0.2.1-py3-none-any.whl size=2887 sha256=833b8da3ae8daff8d63be4f9796408c13ad441caa3c4b5898b89fdca29f60026\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/cb/39/7ce7ff2d2fd37cfe1fe7b3a3c43cf410632b2ad3b3f3986d73\n",
            "Successfully built axial-positional-embedding\n",
            "Installing collected packages: local-attention, axial-positional-embedding, colt5-attention, product-key-memory, reformer_pytorch\n",
            "Successfully installed axial-positional-embedding-0.2.1 colt5-attention-0.11.1 local-attention-1.9.15 product-key-memory-0.2.11 reformer_pytorch-1.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download datasets into the appropriate directory\n",
        "!gdown --folder https://drive.google.com/drive/folders/1ZOYpTUa82_jCcxIdTmyr0LXQfvaM9vIy -O ./dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFtfaIvYMrhA",
        "outputId": "4f62ad9c-2f48-4cbe-cccc-8a2afe9a01e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder contents\n",
            "Retrieving folder 1ZhaQwLYcnhT5zEEZhBTo03-jDwl7bt7v electricity\n",
            "Processing file 1jinfTAApPyuyvW1P1hUDpI3rl0Jq8in1 electricity.csv\n",
            "Retrieving folder 19RDRjf7R46Ag1ZMJMR-MgOEWNtxpRMUb ETT-small\n",
            "Processing file 1vOClm_t4RgUf8nqherpTfNnB8rrYq14Q ETTh1.csv\n",
            "Processing file 1bOcmp9VAv03d3kUYSrttOFvLZ0keXDC5 ETTh2.csv\n",
            "Processing file 1B7VcTWdIfPl3g17zKXATKF9XQJtNHTtl ETTm1.csv\n",
            "Processing file 1JweODeVxt6YTIRFA0ivAgZQkR3rldtbi ETTm2.csv\n",
            "Retrieving folder 1nuMUIADOc1BNN-uDO2N7zohLgpLDgl-Z exchange_rate\n",
            "Processing file 1LHP-2iDgk-kx1vS414HiHeBMhb9vuU4M .DS_Store\n",
            "Processing file 1EBLfP2Dx2K7LsSZybX4JJes-wEcTJbz- exchange_rate.csv\n",
            "Retrieving folder 1DasX30lzEwcVXYaNeyMlQ0PSmCQSow5h illness\n",
            "Processing file 1n4kDxT38rQEGmczIaYaaUWqRPWs-rtcH national_illness.csv\n",
            "Retrieving folder 1M3gTc1DSvnUFMI57p70VFH5MHhZh3wC8 traffic\n",
            "Processing file 1Sd9xCLtRFYwvov52XYjKCUerRWxWmolA .DS_Store\n",
            "Processing file 1U3BZ3Wvuvd9HVAx5Nl3bHYG9rsh5-yZX traffic.csv\n",
            "Retrieving folder 1Xz84ci5YKWL6O2I-58ZsVe42lYIfqui1 weather\n",
            "Processing file 1Tc7GeVN7DLEl-RAs-JVwG9yFMf--S8dy weather.csv\n",
            "Processing file 1alE33S1GmP5wACMXaLu50rDIoVzBM4ik all_six_datasets.zip\n",
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jinfTAApPyuyvW1P1hUDpI3rl0Jq8in1\n",
            "To: /content/Autoformer/dataset/Autoformer/electricity/electricity.csv\n",
            "100% 95.6M/95.6M [00:00<00:00, 138MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1vOClm_t4RgUf8nqherpTfNnB8rrYq14Q\n",
            "To: /content/Autoformer/dataset/Autoformer/ETT-small/ETTh1.csv\n",
            "100% 2.59M/2.59M [00:00<00:00, 206MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bOcmp9VAv03d3kUYSrttOFvLZ0keXDC5\n",
            "To: /content/Autoformer/dataset/Autoformer/ETT-small/ETTh2.csv\n",
            "100% 2.42M/2.42M [00:00<00:00, 123MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1B7VcTWdIfPl3g17zKXATKF9XQJtNHTtl\n",
            "To: /content/Autoformer/dataset/Autoformer/ETT-small/ETTm1.csv\n",
            "100% 10.4M/10.4M [00:00<00:00, 42.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JweODeVxt6YTIRFA0ivAgZQkR3rldtbi\n",
            "To: /content/Autoformer/dataset/Autoformer/ETT-small/ETTm2.csv\n",
            "100% 9.68M/9.68M [00:00<00:00, 69.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LHP-2iDgk-kx1vS414HiHeBMhb9vuU4M\n",
            "To: /content/Autoformer/dataset/Autoformer/exchange_rate/.DS_Store\n",
            "100% 6.15k/6.15k [00:00<00:00, 21.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EBLfP2Dx2K7LsSZybX4JJes-wEcTJbz-\n",
            "To: /content/Autoformer/dataset/Autoformer/exchange_rate/exchange_rate.csv\n",
            "100% 638k/638k [00:00<00:00, 157MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1n4kDxT38rQEGmczIaYaaUWqRPWs-rtcH\n",
            "To: /content/Autoformer/dataset/Autoformer/illness/national_illness.csv\n",
            "100% 67.6k/67.6k [00:00<00:00, 105MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Sd9xCLtRFYwvov52XYjKCUerRWxWmolA\n",
            "To: /content/Autoformer/dataset/Autoformer/traffic/.DS_Store\n",
            "100% 6.15k/6.15k [00:00<00:00, 16.9MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1U3BZ3Wvuvd9HVAx5Nl3bHYG9rsh5-yZX\n",
            "From (redirected): https://drive.google.com/uc?id=1U3BZ3Wvuvd9HVAx5Nl3bHYG9rsh5-yZX&confirm=t&uuid=f51ee4b1-3a9d-40c5-aeba-e0f5bc4abe72\n",
            "To: /content/Autoformer/dataset/Autoformer/traffic/traffic.csv\n",
            "100% 136M/136M [00:01<00:00, 122MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Tc7GeVN7DLEl-RAs-JVwG9yFMf--S8dy\n",
            "To: /content/Autoformer/dataset/Autoformer/weather/weather.csv\n",
            "100% 7.24M/7.24M [00:00<00:00, 57.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1alE33S1GmP5wACMXaLu50rDIoVzBM4ik\n",
            "From (redirected): https://drive.google.com/uc?id=1alE33S1GmP5wACMXaLu50rDIoVzBM4ik&confirm=t&uuid=7025319d-cb69-4970-96f6-ab5541ac1fab\n",
            "To: /content/Autoformer/dataset/Autoformer/all_six_datasets.zip\n",
            "100% 54.0M/54.0M [00:00<00:00, 111MB/s] \n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Move datasets to the expected location\n",
        "!mv ./dataset/Autoformer/* ./dataset/"
      ],
      "metadata": {
        "id": "Mde1RlIe8Svx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9zss0MJpKmF",
        "outputId": "c14a50ac-5061-448a-8705-40e211082db7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "print(torch.cuda.device_count())\n",
        "print(torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "y408H1sipRwr",
        "outputId": "e38757af-aa82-4a42-bb2f-ac7bb458992e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n",
            "0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d2a7478f936a>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_name\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    491\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m     \"\"\"\n\u001b[0;32m--> 493\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_device_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_properties\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0m_CudaDeviceProperties\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mproperties\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m     \"\"\"\n\u001b[0;32m--> 523\u001b[0;31m     \u001b[0m_lazy_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# will define _get_device_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    524\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mdevice_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_script_content = \"\"\"\n",
        "export CUDA_VISIBLE_DEVICES=2\n",
        "\n",
        "python -u run.py \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/ \\\n",
        "  --data_path preprocessed_gold_data.csv \\\n",
        "  --model_id gold_96_24 \\\n",
        "  --model Autoformer \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --target Gold_Price \\\n",
        "  --seq_len 30 \\\n",
        "  --label_len 7 \\\n",
        "  --pred_len 7 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 1 \\\n",
        "  --enc_in 2 \\\n",
        "  --dec_in 2 \\\n",
        "  --c_out 2 \\\n",
        "  --des 'Gold_Price_Volume_Exp' \\\n",
        "  --freq 't' \\\n",
        "  --itr 1\n",
        "\n",
        "python -u run.py \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/ \\\n",
        "  --data_path preprocessed_gold_data.csv \\\n",
        "  --model_id gold_96_48 \\\n",
        "  --model Autoformer \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --target Gold_Price \\\n",
        "  --seq_len 30 \\\n",
        "  --label_len 15 \\\n",
        "  --pred_len 30 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 1 \\\n",
        "  --enc_in 2 \\\n",
        "  --dec_in 2 \\\n",
        "  --c_out 2 \\\n",
        "  --des 'Gold_Price_Volume_Exp' \\\n",
        "  --freq 't' \\\n",
        "  --itr 1\n",
        "\n",
        "python -u run.py \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/ \\\n",
        "  --data_path preprocessed_gold_data.csv \\\n",
        "  --model_id gold_96_96 \\\n",
        "  --model Autoformer \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --target Gold_Price \\\n",
        "  --seq_len 90 \\\n",
        "  --label_len 15 \\\n",
        "  --pred_len 30 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 1 \\\n",
        "  --enc_in 2 \\\n",
        "  --dec_in 2 \\\n",
        "  --c_out 2 \\\n",
        "  --des 'Gold_Price_Volume_Exp' \\\n",
        "  --freq 't' \\\n",
        "  --itr 1\n",
        "\n",
        "python -u run.py \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/ \\\n",
        "  --data_path preprocessed_gold_data.csv \\\n",
        "  --model_id gold_96_288 \\\n",
        "  --model Autoformer \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --target Gold_Price \\\n",
        "  --seq_len 90 \\\n",
        "  --label_len 30 \\\n",
        "  --pred_len 30 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 1 \\\n",
        "  --enc_in 2 \\\n",
        "  --dec_in 2 \\\n",
        "  --c_out 2 \\\n",
        "  --des 'Gold_Price_Volume_Exp' \\\n",
        "  --freq 't' \\\n",
        "  --itr 1\n",
        "\n",
        "python -u run.py \\\n",
        "  --is_training 1 \\\n",
        "  --root_path ./dataset/ \\\n",
        "  --data_path preprocessed_gold_data.csv \\\n",
        "  --model_id gold_96_672 \\\n",
        "  --model Autoformer \\\n",
        "  --data custom \\\n",
        "  --features M \\\n",
        "  --target Gold_Price \\\n",
        "  --seq_len 30 \\\n",
        "  --label_len 7 \\\n",
        "  --pred_len 30 \\\n",
        "  --e_layers 2 \\\n",
        "  --d_layers 1 \\\n",
        "  --factor 1 \\\n",
        "  --enc_in 2 \\\n",
        "  --dec_in 2 \\\n",
        "  --c_out 2 \\\n",
        "  --des 'Gold_Price_Volume_Exp' \\\n",
        "  --freq 't' \\\n",
        "  --itr 1\n",
        "\"\"\"\n",
        "\n",
        "# Save the new script as a file\n",
        "script_path = \"./scripts/train_gold_dataset.sh\"\n",
        "\n",
        "with open(script_path, \"w\") as f:\n",
        "    f.write(new_script_content)\n",
        "\n",
        "# Make the script executable\n",
        "!chmod +x /scripts/train_gold_dataset.sh\n",
        "\n",
        "print(f\"New script saved to {script_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pK01Oux9Iuh",
        "outputId": "a82bd79d-07ca-48ac-c6ed-75e8c0dfac3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chmod: cannot access '/scripts/train_gold_dataset.sh': No such file or directory\n",
            "New script saved to ./scripts/train_gold_dataset.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Load a CSV file\n",
        "df = pd.read_csv('/content/drive/My Drive/Autoformer/Stock_Market_Dataset.csv')\n",
        "\n",
        "# Select the relevant columns\n",
        "columns = ['Date', 'Gold_Price', 'Gold_Vol.']\n",
        "data = df[columns]\n",
        "\n",
        "# Convert date column to datetime\n",
        "data.loc[:, 'date'] = pd.to_datetime(data['Date'],  format=\"%d-%m-%Y\")\n",
        "\n",
        "# Sort by date\n",
        "data = data.sort_values(by='date')\n",
        "\n",
        "# Reset the index\n",
        "data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Fill or drop missing values as needed\n",
        "data.ffill(inplace=True)  # Forward fill\n",
        "data.dropna(inplace=True)  # Alternatively, drop rows with missing values\n",
        "\n",
        "# Initialize a MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Remove commas and convert to numeric\n",
        "data['Gold_Price'] = data['Gold_Price'].astype(str).str.replace(',', '').astype(float)\n",
        "data['Gold_Vol.'] = data['Gold_Vol.'].astype(str).str.replace(',', '').astype(float)\n",
        "\n",
        "# Normalize gold_Price and gold_Vol\n",
        "data[['Gold_Price', 'Gold_Vol.']] = scaler.fit_transform(data[['Gold_Price', 'Gold_Vol.']])\n",
        "\n",
        "# Remove the \"Date\" column\n",
        "data = data.drop(columns=['Date'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7OFSUmKAXi1",
        "outputId": "2973cc1f-6bcb-4c77-f329-65bb882e1a5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-7912ed881152>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  data.loc[:, 'date'] = pd.to_datetime(data['Date'],  format=\"%d-%m-%Y\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to save the preprocessed dataset\n",
        "output_path = './dataset/preprocessed_gold_data.csv'\n",
        "\n",
        "# Save the preprocessed dataset as a CSV file\n",
        "data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Preprocessed dataset saved to: {output_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2FvuupDGHZX",
        "outputId": "d0452c9e-f1f2-478a-d3dc-51238aadd451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed dataset saved to: ./dataset/preprocessed_gold_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash ./scripts/train_gold_dataset.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wu_PbseaN4aa",
        "outputId": "7e4065da-e88a-4bb6-a2f7-7af07a5a58a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-12-01:22:49:10,451 INFO     [utils.py:161] NumExpr defaulting to 2 threads.\n",
            "Args in experiment:\n",
            "Namespace(is_training=1, model_id='gold_96_24', model='Autoformer', data='custom', root_path='./dataset/', data_path='preprocessed_gold_data.csv', features='M', target='Gold_Price', freq='t', checkpoints='./checkpoints/', seq_len=30, label_len=7, pred_len=7, bucket_size=4, n_hashes=4, enc_in=2, dec_in=2, c_out=2, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Gold_Price_Volume_Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3')\n",
            "Use CPU\n",
            ">>>>>>>start training : gold_96_24_Autoformer_custom_ftM_sl30_ll7_pl7_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 834\n",
            "val 119\n",
            "test 242\n",
            "Epoch: 1 cost time: 37.23844027519226\n",
            "Epoch: 1, Steps: 26 | Train Loss: 0.5728725 Vali Loss: 0.3546296 Test Loss: 0.6187415\n",
            "Validation loss decreased (inf --> 0.354630).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 36.14053773880005\n",
            "Epoch: 2, Steps: 26 | Train Loss: 0.4352428 Vali Loss: 0.2705785 Test Loss: 0.5468109\n",
            "Validation loss decreased (0.354630 --> 0.270579).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 34.844104528427124\n",
            "Epoch: 3, Steps: 26 | Train Loss: 0.4058506 Vali Loss: 0.2756572 Test Loss: 0.5072330\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 36.9963059425354\n",
            "Epoch: 4, Steps: 26 | Train Loss: 0.3947976 Vali Loss: 0.2701690 Test Loss: 0.5174514\n",
            "Validation loss decreased (0.270579 --> 0.270169).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 36.842225790023804\n",
            "Epoch: 5, Steps: 26 | Train Loss: 0.3867931 Vali Loss: 0.2764124 Test Loss: 0.5120420\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 36.966832637786865\n",
            "Epoch: 6, Steps: 26 | Train Loss: 0.3858878 Vali Loss: 0.2773247 Test Loss: 0.5140742\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 3.125e-06\n",
            "Epoch: 7 cost time: 37.38646459579468\n",
            "Epoch: 7, Steps: 26 | Train Loss: 0.3808528 Vali Loss: 0.2603537 Test Loss: 0.5083391\n",
            "Validation loss decreased (0.270169 --> 0.260354).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "Epoch: 8 cost time: 36.57470726966858\n",
            "Epoch: 8, Steps: 26 | Train Loss: 0.3824225 Vali Loss: 0.2879880 Test Loss: 0.5063074\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 7.8125e-07\n",
            "Epoch: 9 cost time: 34.36456632614136\n",
            "Epoch: 9, Steps: 26 | Train Loss: 0.3837711 Vali Loss: 0.2662980 Test Loss: 0.5073822\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 3.90625e-07\n",
            "Epoch: 10 cost time: 34.534912109375\n",
            "Epoch: 10, Steps: 26 | Train Loss: 0.3808673 Vali Loss: 0.2971441 Test Loss: 0.5073843\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : gold_96_24_Autoformer_custom_ftM_sl30_ll7_pl7_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 242\n",
            "test shape: (242, 7, 2) (242, 7, 2)\n",
            "test shape: (242, 7, 2) (242, 7, 2)\n",
            "mse:0.5079443454742432, mae:0.5073617696762085\n",
            "2024-12-01:22:56:18,376 INFO     [utils.py:161] NumExpr defaulting to 2 threads.\n",
            "Args in experiment:\n",
            "Namespace(is_training=1, model_id='gold_96_48', model='Autoformer', data='custom', root_path='./dataset/', data_path='preprocessed_gold_data.csv', features='M', target='Gold_Price', freq='t', checkpoints='./checkpoints/', seq_len=30, label_len=15, pred_len=30, bucket_size=4, n_hashes=4, enc_in=2, dec_in=2, c_out=2, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Gold_Price_Volume_Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3')\n",
            "Use CPU\n",
            ">>>>>>>start training : gold_96_48_Autoformer_custom_ftM_sl30_ll15_pl30_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 811\n",
            "val 96\n",
            "test 219\n",
            "Epoch: 1 cost time: 44.06812644004822\n",
            "Epoch: 1, Steps: 25 | Train Loss: 0.7069538 Vali Loss: 0.3154992 Test Loss: 0.5637211\n",
            "Validation loss decreased (inf --> 0.315499).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 45.12520956993103\n",
            "Epoch: 2, Steps: 25 | Train Loss: 0.5218308 Vali Loss: 0.2886701 Test Loss: 0.4999798\n",
            "Validation loss decreased (0.315499 --> 0.288670).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 46.669195890426636\n",
            "Epoch: 3, Steps: 25 | Train Loss: 0.5047453 Vali Loss: 0.2800563 Test Loss: 0.5252487\n",
            "Validation loss decreased (0.288670 --> 0.280056).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 45.76134920120239\n",
            "Epoch: 4, Steps: 25 | Train Loss: 0.4957168 Vali Loss: 0.2813904 Test Loss: 0.5060121\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 44.71926760673523\n",
            "Epoch: 5, Steps: 25 | Train Loss: 0.4931550 Vali Loss: 0.2778912 Test Loss: 0.5028289\n",
            "Validation loss decreased (0.280056 --> 0.277891).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 44.26330780982971\n",
            "Epoch: 6, Steps: 25 | Train Loss: 0.4923314 Vali Loss: 0.2777653 Test Loss: 0.5133898\n",
            "Validation loss decreased (0.277891 --> 0.277765).  Saving model ...\n",
            "Updating learning rate to 3.125e-06\n",
            "Epoch: 7 cost time: 44.40382957458496\n",
            "Epoch: 7, Steps: 25 | Train Loss: 0.4884195 Vali Loss: 0.2759936 Test Loss: 0.5162594\n",
            "Validation loss decreased (0.277765 --> 0.275994).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "Epoch: 8 cost time: 48.55712962150574\n",
            "Epoch: 8, Steps: 25 | Train Loss: 0.4895748 Vali Loss: 0.2766282 Test Loss: 0.5120152\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 7.8125e-07\n",
            "Epoch: 9 cost time: 46.95900011062622\n",
            "Epoch: 9, Steps: 25 | Train Loss: 0.4862093 Vali Loss: 0.2766862 Test Loss: 0.5115634\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 3.90625e-07\n",
            "Epoch: 10 cost time: 45.80756378173828\n",
            "Epoch: 10, Steps: 25 | Train Loss: 0.4882386 Vali Loss: 0.2770281 Test Loss: 0.5104573\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : gold_96_48_Autoformer_custom_ftM_sl30_ll15_pl30_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 219\n",
            "test shape: (219, 30, 2) (219, 30, 2)\n",
            "test shape: (219, 30, 2) (219, 30, 2)\n",
            "mse:0.5150589346885681, mae:0.5353437066078186\n",
            "2024-12-01:23:05:16,253 INFO     [utils.py:161] NumExpr defaulting to 2 threads.\n",
            "Args in experiment:\n",
            "Namespace(is_training=1, model_id='gold_96_96', model='Autoformer', data='custom', root_path='./dataset/', data_path='preprocessed_gold_data.csv', features='M', target='Gold_Price', freq='t', checkpoints='./checkpoints/', seq_len=90, label_len=15, pred_len=30, bucket_size=4, n_hashes=4, enc_in=2, dec_in=2, c_out=2, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Gold_Price_Volume_Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3')\n",
            "Use CPU\n",
            ">>>>>>>start training : gold_96_96_Autoformer_custom_ftM_sl90_ll15_pl30_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 751\n",
            "val 96\n",
            "test 219\n",
            "Epoch: 1 cost time: 82.31710624694824\n",
            "Epoch: 1, Steps: 23 | Train Loss: 0.7199885 Vali Loss: 0.3680215 Test Loss: 0.5810630\n",
            "Validation loss decreased (inf --> 0.368021).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 82.41129231452942\n",
            "Epoch: 2, Steps: 23 | Train Loss: 0.5505024 Vali Loss: 0.3437457 Test Loss: 0.5786776\n",
            "Validation loss decreased (0.368021 --> 0.343746).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 81.96663355827332\n",
            "Epoch: 3, Steps: 23 | Train Loss: 0.5246820 Vali Loss: 0.3254693 Test Loss: 0.5561761\n",
            "Validation loss decreased (0.343746 --> 0.325469).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 82.9104995727539\n",
            "Epoch: 4, Steps: 23 | Train Loss: 0.5214684 Vali Loss: 0.3310221 Test Loss: 0.5576586\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 82.74247527122498\n",
            "Epoch: 5, Steps: 23 | Train Loss: 0.5145858 Vali Loss: 0.3314095 Test Loss: 0.5628933\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 81.92306613922119\n",
            "Epoch: 6, Steps: 23 | Train Loss: 0.5178561 Vali Loss: 0.3349837 Test Loss: 0.5650758\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : gold_96_96_Autoformer_custom_ftM_sl90_ll15_pl30_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 219\n",
            "test shape: (219, 30, 2) (219, 30, 2)\n",
            "test shape: (219, 30, 2) (219, 30, 2)\n",
            "mse:0.5528777241706848, mae:0.5864468812942505\n",
            "2024-12-01:23:15:09,691 INFO     [utils.py:161] NumExpr defaulting to 2 threads.\n",
            "Args in experiment:\n",
            "Namespace(is_training=1, model_id='gold_96_288', model='Autoformer', data='custom', root_path='./dataset/', data_path='preprocessed_gold_data.csv', features='M', target='Gold_Price', freq='t', checkpoints='./checkpoints/', seq_len=90, label_len=30, pred_len=30, bucket_size=4, n_hashes=4, enc_in=2, dec_in=2, c_out=2, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Gold_Price_Volume_Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3')\n",
            "Use CPU\n",
            ">>>>>>>start training : gold_96_288_Autoformer_custom_ftM_sl90_ll30_pl30_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 751\n",
            "val 96\n",
            "test 219\n",
            "Epoch: 1 cost time: 84.58731079101562\n",
            "Epoch: 1, Steps: 23 | Train Loss: 0.7298007 Vali Loss: 0.3454883 Test Loss: 0.5559400\n",
            "Validation loss decreased (inf --> 0.345488).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 88.09677171707153\n",
            "Epoch: 2, Steps: 23 | Train Loss: 0.5436390 Vali Loss: 0.3320664 Test Loss: 0.5960391\n",
            "Validation loss decreased (0.345488 --> 0.332066).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 88.56741905212402\n",
            "Epoch: 3, Steps: 23 | Train Loss: 0.5255476 Vali Loss: 0.3201357 Test Loss: 0.5706212\n",
            "Validation loss decreased (0.332066 --> 0.320136).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 87.09605479240417\n",
            "Epoch: 4, Steps: 23 | Train Loss: 0.5194235 Vali Loss: 0.3250813 Test Loss: 0.5939749\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 86.16005778312683\n",
            "Epoch: 5, Steps: 23 | Train Loss: 0.5139607 Vali Loss: 0.3265889 Test Loss: 0.5818838\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 87.50888276100159\n",
            "Epoch: 6, Steps: 23 | Train Loss: 0.5174812 Vali Loss: 0.3291835 Test Loss: 0.5876489\n",
            "EarlyStopping counter: 3 out of 3\n",
            "Early stopping\n",
            ">>>>>>>testing : gold_96_288_Autoformer_custom_ftM_sl90_ll30_pl30_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 219\n",
            "test shape: (219, 30, 2) (219, 30, 2)\n",
            "test shape: (219, 30, 2) (219, 30, 2)\n",
            "mse:0.5668199062347412, mae:0.592785656452179\n",
            "2024-12-01:23:25:32,523 INFO     [utils.py:161] NumExpr defaulting to 2 threads.\n",
            "Args in experiment:\n",
            "Namespace(is_training=1, model_id='gold_96_672', model='Autoformer', data='custom', root_path='./dataset/', data_path='preprocessed_gold_data.csv', features='M', target='Gold_Price', freq='t', checkpoints='./checkpoints/', seq_len=30, label_len=7, pred_len=30, bucket_size=4, n_hashes=4, enc_in=2, dec_in=2, c_out=2, d_model=512, n_heads=8, e_layers=2, d_layers=1, d_ff=2048, moving_avg=25, factor=1, distil=True, dropout=0.05, embed='timeF', activation='gelu', output_attention=False, do_predict=False, num_workers=10, itr=1, train_epochs=10, batch_size=32, patience=3, learning_rate=0.0001, des='Gold_Price_Volume_Exp', loss='mse', lradj='type1', use_amp=False, use_gpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3')\n",
            "Use CPU\n",
            ">>>>>>>start training : gold_96_672_Autoformer_custom_ftM_sl30_ll7_pl30_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "train 811\n",
            "val 96\n",
            "test 219\n",
            "Epoch: 1 cost time: 43.65691065788269\n",
            "Epoch: 1, Steps: 25 | Train Loss: 0.6666297 Vali Loss: 0.2998261 Test Loss: 0.5217008\n",
            "Validation loss decreased (inf --> 0.299826).  Saving model ...\n",
            "Updating learning rate to 0.0001\n",
            "Epoch: 2 cost time: 42.81818151473999\n",
            "Epoch: 2, Steps: 25 | Train Loss: 0.5190318 Vali Loss: 0.2978568 Test Loss: 0.5549830\n",
            "Validation loss decreased (0.299826 --> 0.297857).  Saving model ...\n",
            "Updating learning rate to 5e-05\n",
            "Epoch: 3 cost time: 42.75348782539368\n",
            "Epoch: 3, Steps: 25 | Train Loss: 0.5090094 Vali Loss: 0.2816805 Test Loss: 0.5064158\n",
            "Validation loss decreased (0.297857 --> 0.281680).  Saving model ...\n",
            "Updating learning rate to 2.5e-05\n",
            "Epoch: 4 cost time: 45.067047119140625\n",
            "Epoch: 4, Steps: 25 | Train Loss: 0.5004510 Vali Loss: 0.2759118 Test Loss: 0.5048254\n",
            "Validation loss decreased (0.281680 --> 0.275912).  Saving model ...\n",
            "Updating learning rate to 1.25e-05\n",
            "Epoch: 5 cost time: 43.047316789627075\n",
            "Epoch: 5, Steps: 25 | Train Loss: 0.4974326 Vali Loss: 0.2746769 Test Loss: 0.5048996\n",
            "Validation loss decreased (0.275912 --> 0.274677).  Saving model ...\n",
            "Updating learning rate to 6.25e-06\n",
            "Epoch: 6 cost time: 42.58172965049744\n",
            "Epoch: 6, Steps: 25 | Train Loss: 0.4951403 Vali Loss: 0.2746941 Test Loss: 0.5078188\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 3.125e-06\n",
            "Epoch: 7 cost time: 43.877986431121826\n",
            "Epoch: 7, Steps: 25 | Train Loss: 0.4916787 Vali Loss: 0.2725725 Test Loss: 0.5044528\n",
            "Validation loss decreased (0.274677 --> 0.272573).  Saving model ...\n",
            "Updating learning rate to 1.5625e-06\n",
            "Epoch: 8 cost time: 42.1730535030365\n",
            "Epoch: 8, Steps: 25 | Train Loss: 0.4936508 Vali Loss: 0.2725387 Test Loss: 0.5042968\n",
            "Validation loss decreased (0.272573 --> 0.272539).  Saving model ...\n",
            "Updating learning rate to 7.8125e-07\n",
            "Epoch: 9 cost time: 43.15407681465149\n",
            "Epoch: 9, Steps: 25 | Train Loss: 0.4899934 Vali Loss: 0.2726719 Test Loss: 0.5054645\n",
            "EarlyStopping counter: 1 out of 3\n",
            "Updating learning rate to 3.90625e-07\n",
            "Epoch: 10 cost time: 45.178651571273804\n",
            "Epoch: 10, Steps: 25 | Train Loss: 0.4937331 Vali Loss: 0.2727151 Test Loss: 0.5050434\n",
            "EarlyStopping counter: 2 out of 3\n",
            "Updating learning rate to 1.953125e-07\n",
            ">>>>>>>testing : gold_96_672_Autoformer_custom_ftM_sl30_ll7_pl30_dm512_nh8_el2_dl1_df2048_fc1_ebtimeF_dtTrue_Gold_Price_Volume_Exp_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
            "test 219\n",
            "test shape: (219, 30, 2) (219, 30, 2)\n",
            "test shape: (219, 30, 2) (219, 30, 2)\n",
            "mse:0.5017865896224976, mae:0.5266557931900024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ./dataset/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOuN8okooGiL",
        "outputId": "70d4c6bc-c9ca-484c-feab-60e8dc88b660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "all_six_datasets.zip  electricity  exchange_rate  preprocessed_gold_data.csv  weather\n",
            "Autoformer\t      ETT-small    illness\t  traffic\n"
          ]
        }
      ]
    }
  ]
}